# üéôÔ∏è Speaker Diarization (Identifica√ß√£o de Quem Est√° Falando)

## üìã Resumo

Speaker Diarization √© uma funcionalidade que identifica automaticamente diferentes pessoas falando em um √°udio, rotulando cada segmento com "Speaker 1", "Speaker 2", etc. √â ideal para entrevistas, podcasts, reuni√µes e qualquer √°udio com m√∫ltiplos interlocutores.

---

## ‚úÖ Implementa√ß√£o no Frontend (CONCLU√çDA)

### 1. **Componente DiarizationToggle**

**Arquivo criado:** [src/components/DiarizationToggle.jsx](src/components/DiarizationToggle.jsx)

**Funcionalidades:**
- ‚úÖ Toggle on/off visual e acess√≠vel
- ‚úÖ Tooltip informativo sobre diarization
- ‚úÖ Explica√ß√£o sobre √°udios est√©reo vs mono
- ‚úÖ Estados disabled durante upload/transcri√ß√£o
- ‚úÖ Design consistente com o resto da aplica√ß√£o

**Exemplo de uso:**
```jsx
<DiarizationToggle
  enabled={diarizationEnabled}
  onToggle={setDiarizationEnabled}
  disabled={isUploading || isTranscribing}
/>
```

---

### 2. **Hook useTranscription Atualizado**

**Arquivo modificado:** [src/hooks/useTranscription.js](src/hooks/useTranscription.js)

**Mudan√ßas:**
```javascript
// Novo estado
const [diarizationEnabled, setDiarizationEnabled] = useState(false);

// Passa diarization para o service
const result = await transcribeAudio(file, language, diarizationEnabled);

// Retorna no hook
return {
  // ... outros estados
  diarizationEnabled,
  setDiarizationEnabled
};
```

---

### 3. **TranscriptionService Atualizado**

**Arquivo modificado:** [src/services/transcriptionService.js](src/services/transcriptionService.js)

**Mudan√ßas:**

```javascript
export const transcribeAudio = async (audioFile, language = 'pt-BR', enableDiarization = false) => {
  console.log('Diarization:', enableDiarization ? 'Ativado' : 'Desativado');

  const formData = new FormData();
  formData.append('file', audioFile);
  formData.append('language', apiLanguage);

  // Enviar par√¢metro de diarization se habilitado
  if (enableDiarization) {
    formData.append('diarization', 'true');
  }

  // ... rest of code
};

// Gerar segmentos com speakers
function generateMockSegments(text, includeSpeakers = false) {
  return sentences.map((sentence, index) => {
    const segment = {
      start: currentTime,
      end: currentTime + duration,
      text: sentence.trim()
    };

    // Adicionar speaker se diarization estiver habilitado
    if (includeSpeakers) {
      segment.speaker = `Speaker ${currentSpeaker + 1}`;
    }

    return segment;
  });
}
```

---

### 4. **TimestampedTranscription Atualizado**

**Arquivo modificado:** [src/components/TimestampedTranscription.jsx](src/components/TimestampedTranscription.jsx)

**Mudan√ßas:**

```jsx
// Detectar se h√° speakers
const hasSpeakers = segments.some(seg => seg.speaker);

// Cores √∫nicas para cada speaker
const speakerColors = {
  'Speaker 1': 'from-blue-500 to-cyan-500',
  'Speaker 2': 'from-purple-500 to-pink-500',
  'Speaker 3': 'from-green-500 to-emerald-500',
  'Speaker 4': 'from-orange-500 to-red-500',
};

// Badge do Speaker
{hasSpeakers && segment.speaker && (
  <div className={`flex items-center gap-1 px-3 py-1 rounded-full text-xs font-semibold bg-gradient-to-r ${speakerColors[segment.speaker]}`}>
    <User className="w-3 h-3" />
    {segment.speaker}
  </div>
)}
```

**Visual:**
- Cada speaker tem uma cor √∫nica (gradiente)
- Badge com √≠cone de usu√°rio
- Aparece ao lado do timestamp

---

### 5. **Exports Atualizados**

**Arquivo modificado:** [src/App.jsx](src/App.jsx)

**Formato TXT com speakers:**
```
[00:15] Speaker 1: Ol√°, bem-vindo ao podcast.
[00:22] Speaker 2: Muito obrigado por me receber!
[00:28] Speaker 1: Vamos come√ßar?
```

**Formato JSON com speakers:**
```json
{
  "text": "Transcri√ß√£o completa...",
  "segments": [
    {
      "start": 15.0,
      "end": 22.0,
      "text": "Ol√°, bem-vindo ao podcast.",
      "speaker": "Speaker 1"
    },
    {
      "start": 22.0,
      "end": 28.0,
      "text": "Muito obrigado por me receber!",
      "speaker": "Speaker 2"
    }
  ],
  "language": "pt-BR",
  "wordCount": 145,
  "charCount": 892,
  "timestamp": "2025-10-23T17:00:00.000Z"
}
```

---

## ‚ö†Ô∏è Necessidade de Upgrade no Backend

### Estado Atual da API

A Edge Function do Supabase (`dynamic-processor`) **atualmente N√ÉO suporta diarization**. O frontend est√° preparado para enviar o par√¢metro `diarization: 'true'`, mas a API precisa ser atualizada para processar isso.

### Op√ß√µes de Implementa√ß√£o no Backend

#### **Op√ß√£o 1: Whisper + Pyannote Audio (Recomendada)**

**Tecnologias:**
- **Whisper** (OpenAI) para transcri√ß√£o
- **Pyannote Audio** para diarization

**Vantagens:**
- ‚úÖ Solu√ß√£o open-source e bem documentada
- ‚úÖ Alta precis√£o em diarization
- ‚úÖ Grande comunidade e suporte
- ‚úÖ Funciona bem com √°udios mono

**Requisitos:**
- GPU com pelo menos 6-8GB VRAM
- Python 3.8+
- Bibliotecas: `openai-whisper`, `pyannote.audio`, `torch`

**Exemplo de implementa√ß√£o:**
```python
import whisper
from pyannote.audio import Pipeline

# Carregar modelos
whisper_model = whisper.load_model("medium")
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")

# Transcrever
transcription = whisper_model.transcribe(audio_path, language=language)

# Diarization
diarization = diarization_pipeline(audio_path)

# Combinar transcri√ß√£o com speakers
segments_with_speakers = combine_transcription_diarization(
    transcription['segments'],
    diarization
)
```

**Refer√™ncias:**
- [Pyannote Audio Docs](https://github.com/pyannote/pyannote-audio)
- [Whisper + Pyannote Tutorial](https://medium.com/@xriteshsharmax/speaker-diarization-using-whisper-asr-and-pyannote-f0141c85d59a)

---

#### **Op√ß√£o 2: WhisperX**

**Tecnologias:**
- **WhisperX** (Whisper com timestamps precisos e diarization integrado)

**Vantagens:**
- ‚úÖ Solu√ß√£o all-in-one
- ‚úÖ Timestamps mais precisos que Whisper vanilla
- ‚úÖ Diarization integrada via Pyannote
- ‚úÖ Alinhamento palavra-por-palavra

**Requisitos:**
- GPU recomendada
- Python 3.8+
- `whisperx` package

**Exemplo:**
```python
import whisperx

device = "cuda"
audio_file = "audio.mp3"
batch_size = 16

# 1. Transcrever com WhisperX
model = whisperx.load_model("large-v2", device, compute_type="float16")
audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)

# 2. Alinhar timestamps
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device)

# 3. Diarization
diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
diarize_segments = diarize_model(audio_file)

# 4. Combinar
result = whisperx.assign_word_speakers(diarize_segments, result)
```

**Refer√™ncias:**
- [WhisperX GitHub](https://github.com/m-bain/whisperX)

---

#### **Op√ß√£o 3: √Åudios Est√©reo (2 canais) - Mais Simples**

Para √°udios est√©reo onde cada canal representa um speaker (ex: entrevistas gravadas com 2 microfones):

**Vantagens:**
- ‚úÖ **100% de acur√°cia** (cada canal = 1 speaker)
- ‚úÖ Processamento mais r√°pido
- ‚úÖ N√£o requer IA de diarization

**Implementa√ß√£o:**
```python
import whisper
from pydub import AudioSegment

# Separar canais
audio = AudioSegment.from_file("stereo_audio.mp3")
left_channel = audio.split_to_mono()[0]
right_channel = audio.split_to_mono()[1]

# Transcrever cada canal
model = whisper.load_model("medium")
transcript_speaker1 = model.transcribe("left_channel.wav")
transcript_speaker2 = model.transcribe("right_channel.wav")

# Combinar com timestamps
combined_segments = merge_by_timestamp(
    transcript_speaker1['segments'],
    transcript_speaker2['segments']
)
```

---

#### **Op√ß√£o 4: APIs Externas**

Se n√£o quiser hospedar os modelos:

**AssemblyAI:**
- API com diarization nativa
- Paga por minuto de √°udio
- F√°cil integra√ß√£o

**Deepgram:**
- API moderna com diarization
- Pricing competitivo
- Boa documenta√ß√£o

**Rev.ai:**
- Focado em transcri√ß√£o profissional
- Suporte a diarization

---

## üöÄ Roadmap de Implementa√ß√£o

### Fase 1: Prepara√ß√£o (Frontend - ‚úÖ COMPLETO)
- [x] Criar componente DiarizationToggle
- [x] Atualizar useTranscription hook
- [x] Modificar transcriptionService
- [x] Atualizar TimestampedTranscription
- [x] Ajustar exports (TXT/JSON)

### Fase 2: Backend (PENDENTE)
- [ ] Decidir abordagem (Pyannote, WhisperX, API externa)
- [ ] Configurar ambiente com GPU (se necess√°rio)
- [ ] Instalar depend√™ncias Python
- [ ] Atualizar Edge Function para aceitar `diarization` param
- [ ] Implementar processamento de diarization
- [ ] Testar com √°udios mono e est√©reo
- [ ] Otimizar performance

### Fase 3: Integra√ß√£o e Testes
- [ ] Testar com √°udios reais
- [ ] Validar precis√£o da diarization
- [ ] Ajustar UI baseado em feedback
- [ ] Documentar limita√ß√µes

### Fase 4: Melhorias Futuras
- [ ] Permitir renomear speakers ("Speaker 1" ‚Üí "Jo√£o")
- [ ] Detectar n√∫mero de speakers automaticamente
- [ ] Configurar n√∫mero m√≠nimo/m√°ximo de speakers
- [ ] Estat√≠sticas por speaker (tempo de fala, % de participa√ß√£o)
- [ ] Exportar apenas fala de um speaker espec√≠fico

---

## üß™ Como Testar Atualmente

### Frontend (Mock)

Enquanto o backend n√£o est√° pronto, o frontend gera speakers simulados para testes:

1. Ative o toggle "Identificar quem est√° falando"
2. Fa√ßa upload de um √°udio
3. A transcri√ß√£o mostrar√° speakers alternados (mock)
4. Baixe TXT ou JSON para ver o formato

**Formato esperado do backend:**

```javascript
{
  "success": true,
  "text": "Transcri√ß√£o completa...",
  "segments": [
    {
      "start": 0.0,
      "end": 5.2,
      "text": "Ol√°, como vai?",
      "speaker": "Speaker 1"  // ‚Üê Adicionar este campo
    },
    {
      "start": 5.5,
      "end": 10.8,
      "text": "Tudo bem, obrigado!",
      "speaker": "Speaker 2"  // ‚Üê Speakers diferentes
    }
  ],
  "language": "pt"
}
```

---

## üìä Casos de Uso

### 1. **Entrevistas**
- 2+ pessoas conversando
- Identificar entrevistador vs entrevistado
- Exportar falas separadas

### 2. **Podcasts**
- M√∫ltiplos hosts
- Convidados
- An√°lise de tempo de fala

### 3. **Reuni√µes**
- Atas de reuni√£o
- Identificar contribui√ß√µes individuais
- Compliance e documenta√ß√£o

### 4. **Aulas e Palestras**
- Professor vs alunos
- Sess√µes de Q&A
- Debates

### 5. **Atendimento ao Cliente**
- Agente vs cliente
- An√°lise de qualidade
- Treinamento

---

## üí° Dicas de Implementa√ß√£o

### Para √Åudios de Alta Qualidade
- Use Pyannote + Whisper large-v2
- Ajuste threshold de diarization
- P√≥s-processamento para remover ru√≠do

### Para Performance
- Use WhisperX com GPU
- Batch processing
- Cache de modelos
- Processamento ass√≠ncrono

### Para √Åudios Desafiadores
- M√∫ltiplos speakers (>4): considere ajustar par√¢metros
- Sobreposi√ß√£o de falas: pode causar imprecis√£o
- Ru√≠do de fundo: pr√©-processamento com noise reduction
- Sotaques diversos: Whisper large-v2 ou v3

---

## üîó Recursos e Refer√™ncias

### Artigos e Tutoriais
- [Speaker Diarization using Whisper ASR and Pyannote (Medium)](https://medium.com/@xriteshsharmax/speaker-diarization-using-whisper-asr-and-pyannote-f0141c85d59a)
- [Whisper and Pyannote: The Ultimate Solution](https://scalastic.io/en/whisper-pyannote-ultimate-speech-transcription/)
- [Building a Custom Audio Transcription Pipeline](https://medium.com/@rafaelgalle1/building-a-custom-scalable-audio-transcription-pipeline-whisper-pyannote-ffmpeg-d0f03f884330)

### Reposit√≥rios GitHub
- [pyannote/pyannote-audio](https://github.com/pyannote/pyannote-audio)
- [m-bain/whisperX](https://github.com/m-bain/whisperX)
- [MahmoudAshraf97/whisper-diarization](https://github.com/MahmoudAshraf97/whisper-diarization)
- [Jose-Sabater/whisper-pyannote](https://github.com/Jose-Sabater/whisper-pyannote)

### Discuss√µes
- [Whisper Discussions #264 - Diarization](https://github.com/openai/whisper/discussions/264)
- [OpenAI Community - Best Diarization Solutions](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922)

---

## ‚úÖ Checklist de Implementa√ß√£o Frontend

- [x] Criar DiarizationToggle component
- [x] Adicionar estado diarizationEnabled no useTranscription
- [x] Passar par√¢metro enableDiarization para transcribeAudio()
- [x] Enviar `diarization: 'true'` no FormData
- [x] Gerar mock segments com speakers
- [x] Exibir badges de speakers no TimestampedTranscription
- [x] Incluir speakers nos exports TXT
- [x] Incluir speakers nos exports JSON
- [x] Integrar DiarizationToggle no App.jsx
- [x] Adicionar cores √∫nicas por speaker
- [x] Documentar funcionalidade

---

**Status do Frontend:** ‚úÖ Implementa√ß√£o completa
**Status do Backend:** ‚ö†Ô∏è Aguardando implementa√ß√£o
**Data:** 2025-10-23
**Vers√£o:** 3.0.0
